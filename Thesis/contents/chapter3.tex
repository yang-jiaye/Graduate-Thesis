\newpage
\fancyhead[LH]{上海交通大学学位论文}
\fancyhead[RH]{第三章\quad 目标跟踪器搭建}
\section{目标跟踪器搭建}

为了实现路侧单元对车辆的对目标跟踪任务，我们采用了EagerMOT框架。由于缺少合适的路侧目标跟踪数据集，我们在KITTI数据集上对该框架进行了性能测试。

\subsection{EagerMOT原理}

EagerMOT框架结合了互补的2D和3D（例如LiDAR）目标信息，这些信息是从预训练的物体检测器中获得的。框架的总体概述如图所示。作为每一帧的输入，我们的方法
采用一组 3D 边界框检测 3dDt 和一组
二维检测 2dDt。 然后，观察融合模块
(i) 将来自 2D 和 3D 的检测关联起来
相同的对象，（ii）两阶段数据关联模块
跨时间关联检测，并且，基于可用的
检测信息（全2D+3D，或部分）我们更新
跟踪状态和（iv）我们采用简单的跟踪管理
初始化或终止轨道的机制。
该公式允许所有检测到的对象与轨迹相关联，即使它们在
图像域或 3D 传感器。 这样，我们的方法就可以
从短遮挡中恢复并保持近似 3D
当其中一个探测器发生故障时的位置，而且重要的是，
我们之前可以在图像域中跟踪远处的物体
物体进入 3D 感应范围。 一旦物体进入
感应范围，我们可以顺利地初始化一个 3D 运动模型
对于每个轨道。

\subsubsection{检测器融合}

a) 获得来自相机的2D视频输入与来自激光雷达3D信息流的检测结果 与 .
b) 对多模态的检测结果进行数据关联，采用贪心算法，将3D检测结果投影到2D图像域上，计算二者的交比(Intersection of Union)。
c) 将所有可能的检测配对按照交比进行排序，按照交比从高到低的顺序，当2D与3D检测结果均尚未配对成功，且交比大于预先设置的阈值(threshold)时，产生融合实例 .
d) 以此得到融合的检测实例集合 .其中，其中，每个实例 包含了来自2D检测器的2D 边界框(bounding box)与来自3D检测器的3D位置与3D边界框，因此 也属于两个单模态检测实例集合 与 , 即 且 
e) 将剩下的未匹配的检测结果 与 作为部分观测结果(partial observation)加入检测实例集合 中与对应的 与 中。
这种思路虽然简单，但是过去的经验证明了它的鲁棒性。
